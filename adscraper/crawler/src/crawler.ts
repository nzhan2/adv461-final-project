import csvParser from 'csv-parser';
import fs from 'fs';
import os from 'os';
import path from 'path';
import { ClientConfig } from 'pg';
import { publicIpv4, publicIpv6 } from 'public-ip';
import { Browser, HTTPRequest, Page } from 'puppeteer';
import puppeteerExtra from 'puppeteer-extra';
import StealthPlugin from 'puppeteer-extra-plugin-stealth';
import sourceMapSupport from 'source-map-support';
import { scrapeAdsOnPage } from './ads/ad-scraper.js';
import { removeCookieBanners } from './pages/cookie-banner-remover.js';
import SubpageExplorer from './pages/find-page.js';
import { PageType, scrapePage } from './pages/page-scraper.js';
import DbClient, { WebRequest } from './util/db.js';
import { InputError, NonRetryableError } from './util/errors.js';
import * as log from './util/log.js';
import { createAsyncTimeout, sleep } from './util/timeout.js';

sourceMapSupport.install();

export interface CrawlerFlags {
  jobId?: number,
  crawlId?: number,
  crawlName: string,
  resumeIfAble: boolean,
  profileId?: string,
  outputDir: string,
  url?: string,
  adId?: number,
  urlList?: string,
  adUrlList?: string,
  logLevel?: log.LogLevel,

  chromeOptions: {
    profileDir?: string,
    headless: boolean,
    executablePath?: string,
    proxyServer?: string
  }

  crawlOptions: {
    shuffleCrawlList: boolean,
    findAndCrawlPageWithAds: number,
    findAndCrawlArticlePage: boolean
    refreshPage: boolean,
    checkpointFreqSeconds?: number,
  }

  scrapeOptions: {
    scrapeSite: boolean,
    scrapeAds: boolean,
    clickAds: 'noClick' | 'clickAndBlockLoad' | 'clickAndScrapeLandingPage',
    screenshotAdsWithContext: boolean,
    captureThirdPartyRequests: boolean
  }
  profileOptions?: any
};

declare global {
  var BROWSER: Browser;
  var FLAGS: CrawlerFlags;
  var CRAWL_TIMEOUT: number;
  var SITE_TIMEOUT: number;
  var PAGE_NAVIGATION_TIMEOUT: number;
  var PAGE_SLEEP_TIME: number;
  var PAGE_SCRAPE_TIMEOUT: number;
  var AD_SLEEP_TIME: number;
  var AD_SCRAPE_TIMEOUT: number;
  var AD_CLICK_TIMEOUT: number;
  var CLICKTHROUGH_TIMEOUT: number;
  var VIEWPORT: { width: number, height: number }
  var CRAWL_ID: number;
}

function setupGlobals(crawlerFlags: CrawlerFlags) {
  globalThis.FLAGS = crawlerFlags;
  // How long the crawler can spend on each item in the crawl list
  // (including scraping ads, landing pages, subpages)
  globalThis.SITE_TIMEOUT = 15 * 60 * 1000;  // 15min
  // How long the crawler should wait for a page to load.
  globalThis.PAGE_NAVIGATION_TIMEOUT = 3 * 60 * 1000;  // 3min
  // How long the crawler should sleep before scraping a page
  globalThis.PAGE_SLEEP_TIME = 5 * 1000;  // 5s
  // How long the crawler can spend scraping the HTML of a page.
  globalThis.PAGE_SCRAPE_TIMEOUT = 2 * 60 * 1000;  // 2min
  // How long the crawler should sleep before scraping/screenshotting an ad
  globalThis.AD_SLEEP_TIME = 5 * 1000;  // 5s
  // How long the crawler can spend scraping the HTML content and screenshot of
  // an ad. Must be greater than |AD_SLEEP_TIME|
  globalThis.AD_SCRAPE_TIMEOUT = 20 * 1000;  // 20s
  // How long the crawler should wait for something to happen after clicking an ad
  globalThis.AD_CLICK_TIMEOUT = 10 * 1000;  // 10s
  // How long the crawler can spend on each clickthrough page
  globalThis.CLICKTHROUGH_TIMEOUT = 60 * 1000;  // 60s
  // Size of the viewport
  globalThis.VIEWPORT = { width: 1366, height: 768 };
  // Default log level is INFO
  globalThis.LOG_LEVEL = crawlerFlags.logLevel ? crawlerFlags.logLevel : log.LogLevel.INFO;
  // Set up log directory based on crawler flag settings. This is a
  // separate function that can be called from other scripts that call crawl()
  // and want to set up logging earlier.
  log.setLogDirFromFlags(crawlerFlags);
}

export async function crawl(flags: CrawlerFlags, pgConf: ClientConfig, checkpointFn?: () => Promise<void>) {
  // Initialize global variables and clients
  // console.log(flags);
  setupGlobals(flags);

  // Validate arguments
  if (!fs.existsSync(flags.outputDir)) {
    log.strError(`Output dir ${flags.outputDir} is not a valid directory`);
    throw new InputError(`Output dir ${flags.outputDir} is not a valid directory`);
  }
  // Check if output directory is writeable. If not, check the file permissions
  // (or mount settings, if running in a container).
  try {
    fs.accessSync(flags.outputDir, fs.constants.R_OK | fs.constants.W_OK);
  } catch (err) {
    log.strError(`Output dir ${flags.outputDir} is not writable`);
    log.info('os.userInfo:');
    log.info(JSON.stringify(os.userInfo()));
    log.info(`os.stat ${flags.outputDir}:`)
    log.info(JSON.stringify(fs.statSync(flags.outputDir)));
    throw new NonRetryableError(`Output dir ${flags.outputDir} is not writable`);
  }

  const db = await DbClient.initialize(pgConf);

  // Read crawl list from args
  let crawlListFile: string = '';
  let crawlList: string[] = [];
  let crawlListAdIds: number[] = [];
  let isAdUrlCrawl = false;

  // Determine how to read and parse the crawl list
  if (flags.url) {
    // Single URL provided
    crawlList = [flags.url];
    if (flags.adId) {
      // URL is an ad URL
      crawlListAdIds = [flags.adId];
      isAdUrlCrawl = true;
    }
  } else if (flags.urlList) {
    // File containing list of URLs provided
    if (!fs.existsSync(flags.urlList)) {
      log.strError(`${flags.urlList} does not exist.`);
      throw new InputError(`${flags.urlList} does not exist.`);
    }
    crawlList = fs.readFileSync(flags.urlList).toString()
      .trimEnd()
      .split('\n')
      .filter((url: string) => url.length > 0);
    crawlListFile = flags.urlList;
  } else if (flags.adUrlList) {
    // File containing list of ad URLs provided
    crawlListFile = flags.adUrlList;
    if (!fs.existsSync(crawlListFile)) {
      log.strError(`${crawlListFile} does not exist.`);
      throw new InputError(`${crawlListFile} does not exist.`);
    }
    try {
      await (new Promise<void>((resolve, reject) => {
        fs.createReadStream(crawlListFile)
          .pipe(csvParser())
          .on('data', data => {
            if (!data.ad_id) {
              reject(new Error('ad_id column missing from adUrlList'));
            }
            if (!data.url) {
              reject(new Error('url column missing from adUrlList'));
            }
            crawlList.push(data.url);
            crawlListAdIds.push(Number.parseInt(data.ad_id));
          }).on('end', () => {
            resolve();
          });
      }));
    } catch (e: any) {
      log.strError(e.message);
      throw new InputError(e.message);
    }
    isAdUrlCrawl = true;
  } else {
    log.strError('Must provide one of the following crawl inputs: url, urlList, or adUrlList');
    throw new InputError('Must provide one of the following crawl inputs: url, urlList, or adUrlList');
  }

  // Validate crawl list urls
  let i = 1;
  for (let url of crawlList) {
    try {
      new URL(url);
    } catch (e) {
      log.strError(`Invalid URL in crawl list ${crawlListFile} at line ${i}: ${url}`);
      throw new InputError(`Invalid URL in crawl list ${crawlListFile} at line ${i}: ${url}`);
    }
  }

  // Now that the length of the crawl list is known, set the global timeout
  // (15 minutes per site in the list)
  globalThis.CRAWL_TIMEOUT = Math.min(2147483647, crawlList.length * 15 * 60 * 1000);

  let crawlListStartingIndex = 0;

  async function createCrawlEntry(): Promise<number> {
    return db.insert({
      table: 'crawl',
      returning: 'id',
      data: {
        job_id: FLAGS.jobId,
        name: FLAGS.crawlName,
        start_time: new Date(),
        completed: false,
        crawl_list: crawlListFile ? crawlListFile : FLAGS.url,
        crawl_list_current_index: 0,
        crawl_list_length: crawlList.length,
        last_checkpoint_index: flags.crawlOptions.checkpointFreqSeconds ? 0 : undefined,
        profile_id: FLAGS.profileId,
        profile_dir: FLAGS.chromeOptions.profileDir,
        crawler_hostname: os.hostname(),
        crawler_ip: await getPublicIp()
      }
    });
  }

  // If a crawl name is passed, determine if we should resume a previous crawl.
  if (FLAGS.crawlName) {
    // First, check if crawl with that name exists
    const prevCrawl = await db.postgres.query('SELECT * FROM crawl WHERE name=$1', [FLAGS.crawlName]);
    let crawlExists = prevCrawl.rowCount && prevCrawl.rowCount > 0;

    // If it does, verify that it can be resumed
    if (crawlExists && FLAGS.resumeIfAble) {
      globalThis.CRAWL_ID = prevCrawl.rows[0].id;
      // Check that the crawl list name is the same
      if (crawlListFile.length != 0) {
        if (path.basename(prevCrawl.rows[0].crawl_list) != path.basename(crawlListFile)) {
          log.strError(`Crawl list file provided does not the have same name as the original crawl. Expected: ${path.basename(prevCrawl.rows[0].crawl_list)}, actual: ${path.basename(crawlListFile)}`);
          throw new InputError(`Crawl list file provided does not the have same name as the original crawl. Expected: ${path.basename(prevCrawl.rows[0].crawl_list)}, actual: ${path.basename(crawlListFile)}`);
        }
      } else {
        if (prevCrawl.rows[0].crawl_list != FLAGS.url) {
          log.strError(`URL provided does not the have same name as the original crawl. Expected: ${prevCrawl.rows[0].crawl_list}, actual: ${FLAGS.url}`);
          throw new InputError(`URL provided does not the have same name as the original crawl. Expected: ${prevCrawl.rows[0].crawl_list}, actual: ${FLAGS.url}`);
        }
      }
      // Check that the crawl list length is the same
      if (prevCrawl.rows[0].crawl_list_length != crawlList.length) {
        log.strError(`Crawl list file provided does not the have same number of URLs as the original crawl. Expected: ${prevCrawl.rows[0].crawl_list_length}, actual: ${crawlList.length}`);
        throw new InputError(`Crawl list file provided does not the have same number of URLs as the original crawl. Expected: ${prevCrawl.rows[0].crawl_list_length}, actual: ${crawlList.length}`);
      }
      // Check if the crawl is already completed
      if (prevCrawl.rows[0].completed) {
        log.info(`Crawl ${CRAWL_ID} (${FLAGS.crawlName}) is already completed, exiting`);
        return;
      }


      // Then assign the crawl id and starting index
      if (FLAGS.crawlOptions.checkpointFreqSeconds && checkpointFn) {
        // If doing a checkpointed crawl, start from checkpoint index.
        log.info(`Resuming crawl ${prevCrawl.rows[0].id} (${FLAGS.crawlName}) from last profile checkpoint index: ${prevCrawl.rows[0].last_checkpoint_index} of ${prevCrawl.rows[0].crawl_list_length}`);
        crawlListStartingIndex = prevCrawl.rows[0].last_checkpoint_index;
      } else {
        // Otherwise use the last recorded index.
        log.info(`Resuming crawl ${prevCrawl.rows[0].id} (${FLAGS.crawlName}) from last crawl index: ${prevCrawl.rows[0].crawl_list_current_index} of ${prevCrawl.rows[0].crawl_list_length}`);
        crawlListStartingIndex = prevCrawl.rows[0].crawl_list_current_index;
      }
      await db.postgres.query('UPDATE crawl SET crawler_hostname=$1, crawler_ip=$2 WHERE id=$3',
        [os.hostname(), await publicIpv4(), CRAWL_ID]);
    } else {
      // If it doesn't exist, then create a new crawl entry with the given name
      globalThis.CRAWL_ID = await createCrawlEntry();
      log.info(`Created new crawl record for ${FLAGS.crawlName} with id ${CRAWL_ID}`);
    }
  } else {
    // If no crawl name is passed, then create a new crawl entry
    globalThis.CRAWL_ID = await createCrawlEntry();
    log.info(`Created new crawl record for ${FLAGS.crawlName} with id ${CRAWL_ID}`);
  }

  // Open browser
  globalThis.BROWSER = await launchBrowser(FLAGS);

  // Set up overall timeout, race with main loop
  let [overallTimeout, overallTimeoutId] = createAsyncTimeout<void>('Overall crawl timeout reached', CRAWL_TIMEOUT);
  try {
    let _crawlLoop = (async () => {

      let lastCheckpointTime = Date.now();
      // Main loop through crawl list
      for (let i = crawlListStartingIndex; i < crawlList.length; i++) {
        if (!BROWSER.connected) {
          throw new Error('Browser disconnected, ending crawl');
        }

        // Check if we need to run the checkpoint function
        if (FLAGS.crawlOptions.checkpointFreqSeconds
          && checkpointFn
          && (Date.now() - lastCheckpointTime) / 1000 > FLAGS.crawlOptions.checkpointFreqSeconds) {
          log.info(`Running checkpoint function at index ${i} in crawl list`);
          await checkpointFn();
          await db.postgres.query('UPDATE crawl SET last_checkpoint_index=$1 WHERE id=$2', [i, CRAWL_ID]);
          log.info(`Successfully saved checkpoint at index ${i}`);
          lastCheckpointTime = Date.now();
        }

        const url = crawlList[i];

        let prevAdId = isAdUrlCrawl ? crawlListAdIds[i] : undefined;

        // Set timeout for this crawl list item
        let [urlTimeout, urlTimeoutId] = createAsyncTimeout(
          `${url}: overall site timeout reached`, SITE_TIMEOUT);

        let seedPage = await BROWSER.newPage();

        try {
          // Set up race with crawl list item timeout
          let _crawl = (async () => {
            try {
              let pageId;
              if (isAdUrlCrawl) {
                pageId = await loadAndHandlePage(url, seedPage, {
                  crawlListIndex: i,
                  pageType: PageType.LANDING,
                  referrerAd: prevAdId,
                  reload: 0
                });
              } else {
                pageId = await loadAndHandlePage(url, seedPage, {
                  crawlListIndex: i,
                  pageType: PageType.MAIN,
                  reload: 0
                });
              }

              if (FLAGS.crawlOptions.refreshPage) {
                await seedPage.close();
                seedPage = await BROWSER.newPage();
                if (isAdUrlCrawl) {
                  pageId = await loadAndHandlePage(url, seedPage, {
                    crawlListIndex: i,
                    pageType: PageType.LANDING,
                    referrerAd: prevAdId,
                    reload: 1
                  });
                } else {
                  pageId = await loadAndHandlePage(url, seedPage, {
                    crawlListIndex: i,
                    pageType: PageType.MAIN,
                    reload: 1
                  });
                }
              }

              let subpageExplorer = new SubpageExplorer();

              // Open additional pages (if specified) and scrape them (if specified)
              if (FLAGS.crawlOptions.findAndCrawlArticlePage) {
                const articleUrl = await subpageExplorer.findArticle(seedPage);
                if (articleUrl) {
                  let articlePage = await BROWSER.newPage();
                  await loadAndHandlePage(articleUrl, articlePage, {
                    crawlListIndex: i,
                    pageType: PageType.SUBPAGE,
                    referrerPageId: pageId,
                    referrerPageUrl: seedPage.url(),
                    reload: 0
                  });
                  await articlePage.close();
                  if (FLAGS.crawlOptions.refreshPage) {
                    articlePage = await BROWSER.newPage();
                    await loadAndHandlePage(articleUrl, articlePage, {
                      crawlListIndex: i,
                      pageType: PageType.SUBPAGE,
                      referrerPageId: pageId,
                      referrerPageUrl: seedPage.url(),
                      reload: 1
                    });
                    await articlePage.close();
                  }
                } else {
                  log.strError(`${url}: Couldn't find article`);
                }
              }

              for (let i = 0; i < FLAGS.crawlOptions.findAndCrawlPageWithAds; i++) {
                const urlWithAds = await subpageExplorer.findHealthRelatedPagesWithAds(seedPage);
                if (urlWithAds) {
                  let adsPage = await BROWSER.newPage();
                  await loadAndHandlePage(urlWithAds, adsPage, {
                    crawlListIndex: i,
                    pageType: PageType.SUBPAGE,
                    referrerPageId: pageId,
                    referrerPageUrl: seedPage.url(),
                    reload: 0
                  });
                  await adsPage.close();
                  if (FLAGS.crawlOptions.refreshPage) {
                    adsPage = await BROWSER.newPage();
                    await loadAndHandlePage(urlWithAds, adsPage, {
                      crawlListIndex: i,
                      pageType: PageType.SUBPAGE,
                      referrerPageId: pageId,
                      referrerPageUrl: seedPage.url(),
                      reload: 1
                    });
                    await adsPage.close();
                  }
                } else {
                  log.strError(`${url}: Couldn't find page with ads`);
                  break;
                }
              }
            } catch (e: any) {
              log.strError('Caught exception in inner crawl loop and timeout block');
              log.error(e, seedPage.url());
            } finally {
              clearTimeout(urlTimeoutId);
            }
          })();
          await Promise.race([_crawl, urlTimeout]);
          await db.postgres.query('UPDATE crawl SET crawl_list_current_index=$1 WHERE id=$2', [i + 1, CRAWL_ID]);
        } catch (e: any) {
          log.strError('Caught exception in site timeout promise race');
          log.error(e, seedPage.url());
        } finally {
          try {
            log.info('Closing tab');
            let [pageCloseTimeout, pageCloseTimeoutId] = createAsyncTimeout('Timed out attempting closing tab after crawl', 120000)
            await Promise.race([seedPage.close(), pageCloseTimeout]);
            clearTimeout(pageCloseTimeoutId);
          } catch (e) {
            log.strError('Caught exception while closing tab');
            throw e;
          }
        }
      }
      await db.postgres.query('UPDATE crawl SET completed=TRUE, completed_time=$1 WHERE id=$2', [new Date(), CRAWL_ID]);
      return;
    })();
    await Promise.race([_crawlLoop, overallTimeout]);
    log.info('Crawl loop completed');
    if (BROWSER.connected) {
      log.info('Browser still connected, closing browser');
      await BROWSER.close();
    }
  } catch (e) {
    log.strError('Caught exeception in crawl timeout promise race');
    if (BROWSER.connected) {
      log.info('Browser still connected, closing browser');
      await BROWSER.close();
    }
    throw e;
  }
}

/**
 * @param pageType Whether the URL is the one in the crawl list, or an
 * additional URL that was found from a link on the initial page.
 * @param referrerPageId The page id of the page that this URL came from,
 * if this is a subpage of the crawl list page.
 * @param referrerPageUrl: The URL of the page that this URL came from.
 * if this is a subpage of the crawl list page.
 */
interface LoadPageMetadata {
  pageType: PageType,
  crawlListIndex: number,
  referrerPageId?: number,
  referrerPageUrl?: string,
  referrerAd?: number,
  reload: number
}

/**
 *
 * @param url URL to visit in the page
 * @param page Tab/Page that the URL should be visited in
 * @param metadata Crawl metadata
 * @returns The page ID of the crawled page in the database
 */
async function loadAndHandlePage(url: string, page: Page, metadata: LoadPageMetadata) {
  log.info(`${url}: Loading page`);
  // if (FLAGS.scrapeOptions.scrapeAds) {
  //   await domMonitor.injectDOMListener(page);
  // }

  // Create an initial entry for the page in the database, to be updated later
  // with the page contents (or any errors encountered)
  const db = DbClient.getInstance();
  const pageId = await db.archivePage({
    timestamp: new Date(),
    job_id: FLAGS.jobId,
    crawl_id: CRAWL_ID,
    original_url: url,
    crawl_list_index: metadata.crawlListIndex,
    page_type: metadata.pageType,
    referrer_page: metadata.referrerPageId,
    referrer_page_url: metadata.referrerPageUrl,
    referrer_ad: metadata.referrerAd,
    reload: metadata.reload
  });

  try {
    // Set up request interception for capturing third party requests
    await page.setRequestInterception(true);
    let requests: WebRequest[] = [];
    const captureThirdPartyRequests = async (request: HTTPRequest) => {
      try {
        // Exit if request capture is disabled
        if (!FLAGS.scrapeOptions.captureThirdPartyRequests) {
          request.continue(undefined, 0);
          return;
        }

        // Exit if request is navigating this tab
        if (request.isNavigationRequest() && request.frame() === page.mainFrame()) {
          request.continue(undefined, 0);
          return;
        }

        // Exclude same origin requests
        if (new URL(request.url()).origin == new URL(page.url()).origin) {
          request.continue(undefined, 0);
          return;
        }

        requests.push({
          timestamp: new Date(),
          job_id: FLAGS.jobId,
          crawl_id: CRAWL_ID,
          parent_page: -1, // placeholder
          initiator: page.url(),
          target_url: request.url(),
          resource_type: request.resourceType(),
        });
        request.continue(undefined, 0);
      } catch (e) {
        log.warning(`${page.url()}: Error handling intercepted request: ${(e as Error).message}`);
        request.continue(undefined, 0);
      }
    };
    page.on('request', captureThirdPartyRequests);

    await page.goto(url, { timeout: globalThis.PAGE_NAVIGATION_TIMEOUT });
    await sleep(PAGE_SLEEP_TIME);
    log.info(`${url}: Page finished loading`);

    // Try to remove all cookie banners that may block content on the page
    await removeCookieBanners(page);

    // Hit "ESC" to try and dismiss any modal popups
    await page.keyboard.press('Escape');

    // Scroll down the page to trigger lazy loading
    await scrollDownPage(page);

    // Scrape the page
    if (FLAGS.scrapeOptions.scrapeSite) {
      await scrapePage(page, {
        pageId: pageId,
        pageType: metadata.pageType,
        referrerAd: metadata.referrerAd
      });
    } else {
      // If not scraping the page, update the contents with the real URL.
      await db.updatePage(pageId, {
        timestamp: new Date(),
        url: page.url(),
      });
    }

    // Scrape ads
    if (FLAGS.scrapeOptions.scrapeAds) {
      await scrapeAdsOnPage(page, {
        crawlListIndex: metadata.crawlListIndex,
        originalUrl: url,
        pageType: metadata.pageType,
        parentPageId: pageId,
      });
    }

    // Save third party requests
    if (FLAGS.scrapeOptions.captureThirdPartyRequests) {
      log.info(`${url}: Saving ${requests.length} same-site and cross-site requests`);
      const db = DbClient.getInstance();
      for (let request of requests) {
        request.parent_page = pageId;
        await db.archiveRequest(request);
      }
    }

    // Disabled this code, because sometimes disabling request interception would hang
    // and cause puppeteer to lose connection to the browser. AFAIK there is no
    // harm in leaving request interception enabled because the page will be
    // closed immediately afterward.
    // // Clean up event listeners
    // log.verbose(`${url}: Cleaning up request listeners`);
    // page.removeAllListeners('request');
    // log.verbose(`${url}: Disabling request interception`);
    // await page.setRequestInterception(false);
    return pageId;
  } catch (e) {
    log.strError('Caught exception in loadAndHandlePage');
    if (e instanceof Error) {
      await db.updatePage(pageId, { error: e.message });
    } else {
      await db.updatePage(pageId, { error: (e as string) });
    }
    throw e;
  }
}

async function scrollDownPage(page: Page) {
  log.info(`${page.url()}: Scrolling page from top to bottom`);
  let innerHeight = await page.evaluate(() => window.innerHeight);
  let scrollY = await page.evaluate(() => window.scrollY);
  let scrollHeight = await page.evaluate(() => document.body.scrollHeight);
  let i = 0;
  // Scroll until at the bottom of the page or 30 iterations pass
  while (scrollY + innerHeight < scrollHeight && i < 30) {
    // set a screen position to scroll from
    let xloc = randrange(50, 100);
    let yloc = randrange(50, 100);

    // Scroll a random amount
    let ydelta = randrange(200, 400);
    // puppeteer provides current mouse position to wheel mouse event
    await page.mouse.move(xloc, yloc);
    await page.mouse.wheel({ deltaY: ydelta });
    await sleep(1000);

    // innerHeight = await page.evaluate(() => window.innerHeight);
    scrollY = await page.evaluate(() => window.scrollY);
    // scrollHeight = await page.evaluate(() => document.body.scrollHeight);
    i += 1;
  }
}

// Launches the browser with the given flags. Called automatically by
// crawl(), so normally you do not need to call this, but it can be called
// directly if the browser needs to be restarted externally
// (e.g. within the checkpoint function).
export async function launchBrowser(flags: CrawlerFlags) {
  log.info('Launching browser...');
  puppeteerExtra.default.use(StealthPlugin())
  let chromeArgs: string[] = ['--disable-dev-shm-usage'];
  if (flags.chromeOptions.proxyServer) {
    chromeArgs.push(`--proxy-server=${flags.chromeOptions.proxyServer}`);
  }
  let browser = await puppeteerExtra.default.launch({
    args: chromeArgs,
    defaultViewport: VIEWPORT,
    headless: flags.chromeOptions.headless,
    handleSIGINT: false,
    userDataDir: flags.chromeOptions.profileDir,
    executablePath: flags.chromeOptions.executablePath
  });
  const version = await browser.version();
  log.info('Running ' + version);
  return browser;
}

function randrange(low: number, high: number): number {
  return Math.random() * (high - low) + low;
}

async function getPublicIp() {
  try {
    let v4 = await publicIpv4();
    if (v4) {
      return v4;
    }
  } catch (e) {
    log.error(e as Error);
    try {
      let v6 = await publicIpv6();
      if (v6) {
        return v6;
      }
    } catch (e) {
      log.error(e as Error);
      return null;
    }
  }
}
